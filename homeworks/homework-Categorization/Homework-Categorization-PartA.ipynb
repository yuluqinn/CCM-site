{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Categorization and Model Comparison Part A (70/110 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Todd Gureckis* and *Brenden Lake*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "  This homework is due before midnight on April 25 2022. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as str\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import random, randint, shuffle, uniform\n",
    "from scipy.optimize import fmin, brute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background and Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we explore the cognitive mechanisms that support unsupervised pattern categorization in humans.  In addition, we use this as an example of testing and comparing between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple (classic) unsupervised categorization experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Posner and Keele (1968) report a now classic categorization experiment with humans.  In the task participants viewed visual stimuli that are clouds of points (known as dot patterns) similar to a scatter plot of data on a graph.  An examples of the stimuli is shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/singledotpattern.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment Design**  \n",
    "\n",
    "The experiment was divided into a training and test phase.  During the training phase, for each subject a single random dot pattern was generated and considered to be the underlying \"prototype\" structure.  A prototype is like a common template or reference pattern.  The key is that participants never get to see the \"prototype\" pattern directly during training.  Instead they see what are known as \"distortions\" of the prototype.  A distortion of a pattern is made by adding random spatial noise to each point in a pattern to kind of \"wiggle\" the points away from their original position.  \n",
    "\n",
    "For example, here is a random prototype (top) and a bunch of random distortions of the prototype made by adding or subtracting small random values from the `<x,y>` value of each point in the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/distortions.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posner and Keele created distortions that added more or less random noise.  For example, \"high\" distortions add a lot of randomness to the underlying template pattern whereas \"low\" distortions add only a little bit of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Phase**  \n",
    "\n",
    "In the training phase of the experiment subjects view 10 training examples one at a time which are \"high\" distortions of a randomly generated prototype.  The instructions are that subjects should look at these patterns, and that they come from a single category similar to if you viewed a series of pictures of dogs they would all come from the category ``dog``.  Subjects were try to figure out the pattern that related the different images to one another.\n",
    "Try it for yourself by looking at each of the \"distortions\" patterns above one by one and trying to detected the common structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Phase** \n",
    "\n",
    "During the test phase, participants view a series of dot patterns one at a time and have to judge: **Does the given pattern come from the same general category or family you studied earlier or is it a new pattern that is different?**  This is an unsupervised categorization task because the subject has to abstract what the common structure is from the given patterns and then use that information to make classification decisions about new patterns.\n",
    "\n",
    "Unknown to participants the set of test items varied in a specific way with respect the training patterns.  In particular, there were five particular types of patterns presented during test.\n",
    "\n",
    "- The first type were \"old\" patterns which were identical to those presented during the training phase.\n",
    "- The second type were \"random\" patterns which were from a complete new randomly generated prototype (thus had nothing to do with the items presented during training).\n",
    "- The third type were new \"high distortions\" of the underlying prototype that was used to create the study set.  These are thus similar to the \"old\" items but do not match exactly.\n",
    "- The fourth type were \"low distortions\" of the underlying prototype that was used to crete the study set.  These are more similar to the prototype pattern than the \"high\" distortions are.\n",
    "- Finally the actual prototype used to generate the items during training was presented.  This pattern is interesting because the prototype pattern was never seen exactly during training.  However, people saw many high distortions of this item during training and given the instructions to detect what the common structure of the training patterns is, they may have learned something about this latent or hidden pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typical Results**:  \n",
    "\n",
    "This graph show example results that are typical for an experiment like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Stimulus Type\":['Prototype','Low','High','Random','Old'],\"Probability of Endorsement\":[0.91,0.76,0.5,0.27,0.79]})\n",
    "sns.barplot(x=\"Stimulus Type\",y=\"Probability of Endorsement\",data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The height of the bars indicates the probability of endorsing a pattern as a member of the category during the test phase (high values mean that at test a subject is more likely to agree \"yes this pattern fits with the one I learned\").  Notice that the \"old\" items (the exact patterns studied during learning are endorsed at a relatively high rate.  In contrast the \"random\" patterns (those coming from a completely different underlying pattern) are endorsed at a very low rate.  The low and high distortions are endorsed at intermediate rates (with the low distoritions preferred).  Interestingly, the prototype pattern is endorsed most strongly even though it was never presented during the study period!  It is like during learning people figured out the underlying pattern that generated the stimuli!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorization: Memory for examples or abstractions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot pattern experiments are interesting and have been replicated in various ways perhaps hundreds of time.  How do people make these judgments?  What information to people store during the study phase that would predict their performance in the test phase?  What rule do they use for combining information from memory in order to make these kind of judgements?  We are going to build up a model of categorization in a few simple steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: How are dot patterns represented in the mind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first question concerns how people represent the dot patterns and the similarity between different dot patterns.  A variety of work has suggested that the psychological similarity between pairs of dot patterns follows roughly a logarithmic transform of the average euclidean distance between the pairs of points (plus one).  This was established by having people view pairs of dot patterns and rate how similar they seem.  For example here is a plot from a paper by Smith and Minda showing a strong contgruence between dissimiliarty ratings between pairs of stimuli and `log(distance)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dotsimilaritylog.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of this lets define the similarity between two dot patterns $i$ and $j$ as $s_{ij}$ and let it equal the following equation:  \n",
    "\n",
    "<br>\n",
    "<center>\n",
    "$\\Large s_{ij} = log(1 + \\frac{\\sqrt{\\sum_d[(i_{d_x}-j_{d_x})^2 + (i_{d_y}-j_{d_y})^2]}}{9})$\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "where $i_{d_x}$ is the $x$ position of the $d$-th dot in pattern $i$ and $j_{d_x}$ is the $x$ position for pattern $j$ (likewise for $i_{d_y}$). There are 9 dots total, so we divide by 9 to get the average. Because it can sometimes be ambiguous which dot aligns which which one in a pattern we choose the dots which are closest in the two patterns to compute this score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: What is stored in memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next consideration is what people actually store in memory during the training phase of the experiment.  There are of course many alternatives.  People could store an \"average\" of the points seen so far, or they could store each individual pattern that they have seen, or they could store nothing and try to figure it out at test, or they could store some verbal description of what the shapes \"look like\", or the shape of the outer edge of the dot-cloud (the \"convex null\"), etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, however, two leading theories which have attracted considerable debate in the cognitive science literature: the prototype and exemplar theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplar models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplar models are a general class of psychological models related to nearest neighbor algorithms.  The most important feature of these models is the idea that people have what appears to be a nearly infitite memory for the past and as a result you can store all past experiences or examples in memory.  This seems crazy as we are forgetting things all the time but actually psychology is unclear about if we actually forget things or if we simply lose the ability to retreive a memory (i.e., more like losing the pointer to the memory rather than decay).  \n",
    "\n",
    "As mentioned in lecture, nearset neighbor classifiers use a similarity function (similar to the ones described above) to retrieve from memory the nearest labeled example and to predict the category membership based on the label for this item.  This nearest neighbor algorithm can be relaxed slightly to consider $k$-nearest neighbors.  According to this algorithm you find the $k$ neighbor examples (with $k>1$) to the current pattern and response based on what the majority of these examples say.  \n",
    "\n",
    "Now we can go a bit further and say that you compute the similarity to all past examples but *weight* their vote according to their similarity.  So instead of picking the label of the closest or $k$-closest examples we compare the current pattern using a global match to all examples in the memory and weight their predictions based on similarity.  Pretty neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but how does this help us in the case of **unsupervised** categorization such as in the dot pattern case?  Here what we will assume is that we compute this similarity of the to-be-categorized item (the test item) to all the examples stored in memory and compare it to some criterion value.  If the sum of the similarity to all the examples falls below this criterion then we assume the pattern is new and doesn't match what we learned.  If it is above the criterion we judge the item is a good example of the category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example model we will consider the probability of endorsing an item is going to be determined by the following equation:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "$\\Large P(A|i) = \\frac{\\sum_j e^{-c \\cdot s_{ij}}}{\\sum_j e^{-c \\cdot s_{ij}} + k}$\n",
    "</center>\n",
    "</br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $P(A|i)$ is the probability of endorsing pattern $i$ as a member of the category seen during study. $s_{ij}$ is the similarity between pattern $i$ and pattern $j$ which is an example stored in memory during the study phase.  $k$ is the criterion against which the summed similarity is being compared.  If $k$ is zero then you endorse the item as a member of the category all the time irrespecitive of the similiary and if $k$ gets large you become more and more less likely to endorse the item (i.e., more likely to say no).\n",
    "\n",
    "<img src=\"images/exemplarcompare.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum is with respect to an exponential sum which has some deeper relation to research on categorization that we do not have time to discuss.  However, it is basically the idea that very close matches ($s_{ij}=0$) are especially strong and things that are less similar count less.  You can think of it as the the particular weighted nearest neighbor algorithm we think the mind uses. $c$ is a free parameter that controls that weighting function and is often fitted to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prototype model is different than the exemplar model because it assumes that instead of storing each of the training patterns in memory exactly, instead people store a single summary representation.  For example, people might store a mentally computed \"average\" pattern.  When you think about how you would perform the task you might think that you kind of compare the training patterns to one another and then compute some summary.\n",
    "\n",
    "<img src=\"images/prototypetheory.png\" width=\"500\">\n",
    "\n",
    "In the case of the dot pattern stimuli one way to do this is to store a special trace in memory called the prototype which is the average of all the patterns seen so far (averaging the $<x,y>$ position of each point to find an average dot location.\n",
    "\n",
    "According to the prototype model the probabililty of endorsing a test item pattern as a member of the category that was studied during training is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center>\n",
    "$\\Large P(A|i) = \\frac{e^{-c \\cdot s_{ip}}}{e^{-c \\cdot s_{ip}} + k}$\n",
    "</center>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nearly everything about this equation is the same except there is no longer a sum!  Instead we simply compute the similarity between the test pattern and this special \"prototype\" pattern ($p$) which has been averaged from the training examples.\n",
    "\n",
    "<img src=\"images/prototypecomparison.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$ and $c$ are \"free parameters\" in both the exemplar and prototype model which are assumed to modulate or alter the core psychological processes.  These parameters might vary between subjects and as a function of condition.  Thus, in order to assess the ability of the model to account for the data we often \"fit\" these parameters to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these idea in mind, in this homework you are going to compare the exemplar and prototype model to account for some data from an actual dot pattern categorization task collected with human subjects.  The goal is that by doing the homework you would develop some useful code that would let you more or less plug in a model that you might come across in your research, fit it to data, and verify that the fits are good, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data/` folder that comes with this homework contains data from 14 human subjects who participated in a dot pattern classification task.  The data describing each subject is in a text file (`.dat`) indexed by subject number (e.g., `1.dat`, `2.dat`, etc...).\n",
    "\n",
    "The organization of these files is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 44 lines of the file contain a description of the stimulus that the subject saw on a given trial including the x, y coordinate of each dot.  The first columns of these 44 lines is the number of the pattern (`1-44`).  The second column is the type of pattern using the following codes:\n",
    "\n",
    "- `1` = prototype\n",
    "- `2` = 10 \"high distortions\" of the prototype that were used as study patterns during learning\n",
    "- `3` = 10 new \"high distortions\" of the prototype presented during test\n",
    "- `4` = 4 \"low distortions\" of the prototype that were presented during test\n",
    "- `5` = 20 random items that come from different prototypes that were presented at test\n",
    "\n",
    "The next 18 values of each row are the coordinates of the dots (with the x, y coordinates in sequence).  So `[x1, y1, x2, y2, x3, y3, ...]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 40 lines of the file show the sequence of items presented during the study phase.  This is not all that important for our purposes, but basically the last column is which pattern was displayed (indexed from the patterns just described.  Each of 10 \"high distortions\" were presented four times each during study in a random order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the remaining lines of the file report the results of the test phase.  The first column is the subject number, the second columns is the condition number, the next is the trial number in the experiment, the other columns worth mentioning are the last column (the pattern number from the beginning of the file), the second to last column (the type of stimulus it is according to the codes described above), and the reaction time in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, participants in this experiment were assigned to one of two conditions: a recognition condition and a categorization condition.  These conditions differed only in the instructions given to participants at the start of the test phase.  In the recognition condition participants were told they would view a series of patterns and they should respond \"yes\" only if the patterns was **exactly** one they say in the previous study phase.  In the categorization condition, participants were asked to respond \"yes\" only if the pattern belonged to the same general category or pattern that they observed in the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph computes the probability of endorsement in the data set as a function of stimulus type and condition (Cat or Rec instructions).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# getcurve\n",
    "################################\n",
    "def getcurve(filename):\n",
    "    prototypes = []\n",
    "    low = []\n",
    "    old = []\n",
    "    high = []\n",
    "    random = []\n",
    "    mydata = readfile(filename)\n",
    "    cond = mydata[-1][1]\n",
    "    for line in mydata:\n",
    "        if line[4] == 2 and len(line) == 9:\n",
    "            if line[7] == 1:\n",
    "                prototypes.append(line[5])\n",
    "            if line[7] == 2:\n",
    "                old.append(line[5])\n",
    "            if line[7] == 3:\n",
    "                high.append(line[5])\n",
    "            if line[7] == 4:\n",
    "                low.append(line[5])\n",
    "            if line[7] == 5:\n",
    "                random.append(line[5])\n",
    "\n",
    "    #print([len(prototypes), len(low), len(high), len(random), len(old)])\n",
    "    # print(prototypes)\n",
    "    # print(low)\n",
    "    # print(high)\n",
    "    # print(random)\n",
    "    # print(old)\n",
    "    return [np.average(prototypes), np.average(low), np.average(high), np.average(random), np.average(old), filename, cond]\n",
    "\n",
    "\n",
    "def readfile(filename):\n",
    "    results = []\n",
    "    fp = open(filename, 'r')\n",
    "    for line in fp.readlines():\n",
    "        myline = list(map(int, line.split(' ')[:-1]))\n",
    "        results.append(myline[:])\n",
    "    fp.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_all_filenames(directoryname):\n",
    "    files = filter(lambda x: x[-4:] == '.dat' and x[0] !=\n",
    "                   '.', os.listdir(os.path.join(\".\", directoryname)))\n",
    "    fn = map(lambda x: os.path.join(\".\", directoryname, x), files)\n",
    "    # process each file and drop last 5 trials\n",
    "    return list(fn)\n",
    "\n",
    "\n",
    "def create_df(subjnum, cond, pattern):\n",
    "    nobs = len(pattern)\n",
    "    df = pd.DataFrame({\"Subject\": [subjnum]*nobs, \"Condition\": [cond]*nobs, \"Stimulus Type\": [\n",
    "                      'Prototype', 'Low', 'High', 'Random', 'Old'], \"Probability of Endorsement\": pattern})\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_human_results():\n",
    "    allres = map(getcurve, get_all_filenames('data'))\n",
    "    cat = []\n",
    "    rec = []\n",
    "    for patt in allres:\n",
    "        if patt[-1] == 0:\n",
    "            cat.append(create_df(patt[-2], 'cat', patt[:-2]))\n",
    "        else:\n",
    "            rec.append(create_df(patt[-2], 'rec', patt[:-2]))\n",
    "    cat, rec = pd.concat(cat), pd.concat(rec)\n",
    "    return pd.concat([cat, rec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Stimulus Type\", y=\"Probability of Endorsement\", hue=\"Condition\", data=get_human_results())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (20 points) </h3><br>\n",
    "Using your own words explain the data pattern you see in the above figure.  What is different between the conditions and stimulus type?  Why do you suspect these patterns exist?  Your answer will need to consider the nature of the experiments, what is manipulated, and even your intuitive psychological theory about what might be going on.  Your response should take 3-4 sentences and appear in a cell below.  Is any feature of this data surprising to you?\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the exemplar model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells set up the exemplar model using the equations described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# unitdist:\n",
    "# computes the euclidean distance between\n",
    "# two dots\n",
    "################################\n",
    "def unitdist(x, y):\n",
    "    x1 = np.array(x)\n",
    "    y1 = np.array(y)\n",
    "    return math.sqrt(sum(pow(x-y, 2.0)))\n",
    "\n",
    "\n",
    "################################\n",
    "# computeresponse\n",
    "# computes the \"activation\" of each\n",
    "# trace in memory\n",
    "################################\n",
    "def computeresponse(target, memory, c, k):\n",
    "    res = []\n",
    "    for mem in memory:\n",
    "        res.append(\n",
    "            math.log(1.0+np.average(list(map(lambda x, y: unitdist(x, y), target, mem)))))\n",
    "    resp = [math.exp(-c*x) for x in res]\n",
    "    pofr = sum(resp)/(sum(resp)+k)\n",
    "    return pofr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# exemplar model\n",
    "# stores all 10 study items in memory\n",
    "# and computes the probability of endorsement\n",
    "# for each item type\n",
    "################################\n",
    "\n",
    "\n",
    "def exemplarmodel(filename, c, k):\n",
    "    data = readfile(filename)\n",
    "    cond = data[-1][1]\n",
    "    memory = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 2:\n",
    "            memory.append(np.resize(line[2:], (9, 2)))\n",
    "    # print(memory)\n",
    "\n",
    "    # prototype items\n",
    "    proto = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 1:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            proto.append(pofr)\n",
    "    # print(np.average(proto))\n",
    "\n",
    "    # old items\n",
    "    old = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 2:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            old.append(pofr)\n",
    "    # print \"p of r\", old\n",
    "    # print(np.average(old))\n",
    "\n",
    "    # new high items\n",
    "    newhigh = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 3:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            newhigh.append(pofr)\n",
    "    # print(np.average(newhigh))\n",
    "\n",
    "    # new low items\n",
    "    newlow = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 4:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            newlow.append(pofr)\n",
    "    # print(np.average(newlow))\n",
    "\n",
    "    # random items\n",
    "    random = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 5:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            random.append(pofr)\n",
    "    # print(np.average(random))\n",
    "\n",
    "    return [np.average(proto), np.average(newlow), np.average(newhigh), np.average(random), np.average(old), filename, cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exemplar_results(c_cat, k_cat, c_rec, k_rec):\n",
    "    allres = {fn: readfile(fn) for fn in get_all_filenames('data')}\n",
    "    cat = []\n",
    "    rec = []\n",
    "    for filename in allres.keys():\n",
    "        if allres[filename][-1][1] == 0:\n",
    "            res = exemplarmodel(filename, c_cat, k_cat)\n",
    "            cat.append(create_df(filename, 'cat', res[:-2]))\n",
    "        else:\n",
    "            res = exemplarmodel(filename, c_rec, k_rec)\n",
    "            rec.append(create_df(filename, 'rec', res[:-2]))\n",
    "    cat, rec = pd.concat(cat), pd.concat(rec)\n",
    "    return pd.concat([cat, rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's replot the human results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Stimulus Type\", y=\"Probability of Endorsement\", hue=\"Condition\", data=get_human_results())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (10 points) </h3><br>\n",
    "By hand adjust the setting of the model parameters to roughly fit the human data pattern shown above.  How close can you get?  What parameters did you find (report them) and you assessmnet of how well they fit.  Was it a good fit or are there systematic problems with the fit?  In addition, what are the parameter values and do they make sense in light of the equations described above?  When the parameters are the same for recognition and categorization instructions why do the bars look a little different?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell lets you plot the model predictions for the exemplar model fitted to the stimuli that participants in this experiment actually viewed.  There is a $k$ and a $c$ parameter for both categorization and recogniton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_cat, k_cat, c_rec, k_rec = 2.0, 2.0, 2.0, 2.0\n",
    "sns.barplot(x=\"Stimulus Type\", y=\"Probability of Endorsement\",\n",
    "            hue=\"Condition\", data=get_exemplar_results(c_cat, k_cat, c_rec, k_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the prototype model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells set up the prototype model using the equations described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# prototype model\n",
    "# stores an averate of the study items in memory\n",
    "# and computes the probability of endorsement\n",
    "# for each item type\n",
    "################################\n",
    "def prototypemodel(filename, c, k):\n",
    "    data = readfile(filename)\n",
    "    cond = data[-1][1]\n",
    "    # average all the old items in memory\n",
    "    memory = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 2:\n",
    "            memory.append(line[2:])\n",
    "    memory = [\n",
    "        np.resize(list(map(np.average, np.transpose(np.array(memory)))), (9, 2))]\n",
    "\n",
    "    # prototype items\n",
    "    proto = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 1:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            proto.append(pofr)\n",
    "    # print(np.average(proto))\n",
    "\n",
    "    # old items\n",
    "    old = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 2:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            old.append(pofr)\n",
    "    # print \"p of r\", old\n",
    "    # print(np.average(old))\n",
    "\n",
    "    # new high items\n",
    "    newhigh = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 3:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            newhigh.append(pofr)\n",
    "    # print(np.average(newhigh))\n",
    "\n",
    "    # new low items\n",
    "    newlow = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 4:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            newlow.append(pofr)\n",
    "    # print(np.average(newlow))\n",
    "\n",
    "    # random items\n",
    "    random = []\n",
    "    for line in data:\n",
    "        if len(line) == 20 and line[1] == 5:\n",
    "            item = np.resize(line[2:], (9, 2))\n",
    "            pofr = computeresponse(item, memory, c, k)\n",
    "            random.append(pofr)\n",
    "    # print(np.average(random))\n",
    "\n",
    "    return [np.average(proto), np.average(newlow), np.average(newhigh), np.average(random), np.average(old), filename, cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prototype_results(c_cat, k_cat, c_rec, k_rec):\n",
    "    allres = {fn: readfile(fn) for fn in get_all_filenames('data')}\n",
    "    cat = []\n",
    "    rec = []\n",
    "    for filename in allres.keys():\n",
    "        if allres[filename][-1][1] == 0:\n",
    "            res = prototypemodel(filename, c_cat, k_cat)\n",
    "            cat.append(create_df(filename, 'cat', res[:-2]))\n",
    "        else:\n",
    "            res = prototypemodel(filename, c_rec, k_rec)\n",
    "            rec.append(create_df(filename, 'rec', res[:-2]))\n",
    "    cat, rec = pd.concat(cat), pd.concat(rec)\n",
    "    return pd.concat([cat, rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again lets replot the human results for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Stimulus Type\", y=\"Probability of Endorsement\", hue=\"Condition\", data=get_human_results())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 3 (10 points) </h3><br>\n",
    "By hand adjust the setting of the model parameters in the next cell to roughly fit the human data pattern shown above.  How close can you get?  What parameters did you find (report them) and you assessment of how well they fit.  Was it a good fit or are there systematic problems with the fit?  In addition, what are the parameter values and do they make sense in light of the equations described above?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_cat, k_cat, c_rec, k_rec = 2.0, 2.0, 2.0, 2.0\n",
    "res=get_prototype_results(c_cat, k_cat, c_rec, k_rec)\n",
    "sns.barplot(x=\"Stimulus Type\", y=\"Probability of Endorsement\", hue=\"Condition\", data=res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the models using RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we would like to come up with a more quantitative way to assess the quality of the model fits.  The first technique we will use is the \"goodness of fit\" measures that were discussed in lecture.  One of the most common measures of goodness of fit is the Root Mean Squared Error (RMSE).  This measure compares the value of each data point $x$ to each prediction $y$ using the following equation:\n",
    "\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "$\\Large RMSE = \\sqrt{\\frac{\\sum_i (x_i - y_i)^2}{N}}$\n",
    "</center>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often the RMSE is computed between the AVERAGE prediction of the model and the AVERAGE estimates of the behavior to all the subjects in an experiment.  Using the code we developed above we can find the average endorsement curves for humans and both models like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_results=get_human_results()\n",
    "exemplar_predictions = get_exemplar_results(c_cat, k_cat, c_rec, k_rec)\n",
    "prototype_predictions = get_prototype_results(c_cat, k_cat, c_rec, k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avghuman=human_results.groupby(['Condition', 'Stimulus Type'],as_index=False).mean()\n",
    "avghuman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgexemplar=exemplar_predictions.groupby(['Condition', 'Stimulus Type'],as_index=False).mean()\n",
    "avgexemplar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgprototype=prototype_predictions.groupby(['Condition', 'Stimulus Type'], as_index=False).mean()\n",
    "avgprototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 4 (20 points) </h3><br>\n",
    "First, write a function below called `rmse` that computes the RMSE between two `numpy` vectors.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(human, model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your code above.  This code will then by used in the provided functions below to evaluate the fit of the prototype and exemplar models.  The parameters to the model is provided as a list with `[c_cat, k_cat, c_rec, k_rec]` the implied order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_exemplar_model_rmse(params, human_results):\n",
    "    [c_cat, k_cat, c_rec, k_rec] = params\n",
    "    predictions = get_exemplar_results(c_cat, k_cat, c_rec, k_rec)\n",
    "    avgpredict=predictions.groupby(['Condition', 'Stimulus Type'],as_index=False).mean()\n",
    "    model_results = avgpredict['Probability of Endorsement'].values\n",
    "    return rmse(human_results, model_results)\n",
    "\n",
    "def fit_prototype_model_rmse(params, human_results):\n",
    "    [c_cat, k_cat, c_rec, k_rec] = params\n",
    "    predictions = get_prototype_results(c_cat, k_cat, c_rec, k_rec)\n",
    "    avgpredict=predictions.groupby(['Condition', 'Stimulus Type'],as_index=False).mean()\n",
    "    model_results = avgpredict['Probability of Endorsement'].values\n",
    "    return rmse(human_results, model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_results=get_human_results()\n",
    "avghuman=human_results.groupby(['Condition', 'Stimulus Type'],as_index=False).mean()\n",
    "human_results = avghuman['Probability of Endorsement'].values\n",
    "\n",
    "print(fit_exemplar_model_rmse([0.1, 2.0, 0.1, 2.0], human_results))\n",
    "print(fit_prototype_model_rmse([0.1, 2.0, 0.1, 2.0], human_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "Next adjust the parameters by hand for both the exemplar and prototype models to find values that appear to minimize the RMSE.  Copy the code above for plotting the predictions of the models given your best fit parameters.   Which model do you think fits better according to this fit statistic?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 5 (10 points) </h3><br>\n",
    "Read about the scipy `fmin` function.  Use fmin to algorithmically search for the best parameters for each model using the RMSE evaluation function described above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "0.22.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
